{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5a5529-bc6d-4e99-97f1-7e94fc785f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b86e9c-d177-466c-bf8e-85e66b905f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32ae6ee8-89a6-486b-b396-acdcf6478f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import our main reference file\n",
    "'''\n",
    "main_df = pd.read_csv('C:/Users/Desktop/Work/Platform Migration/Migration Tracker/111623_Reference Split.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68d5f46a-9357-45cd-9af1-e67dd400579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, name, reference_id, accountno, reference]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "First, we need to identify and handle NA values in our df.\n",
    "'''\n",
    "# Identify rows where 'reference_id' is not numeric\n",
    "non_numeric_rows = main_df[pd.to_numeric(main_df['reference_id'], errors='coerce').isna()]\n",
    "\n",
    "# Display the rows with non-numeric 'reference_id'\n",
    "print(non_numeric_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b7100f7-d2a8-401c-879a-6788a7844d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After identifying the problematic rows, tt looks like the main issue is they're just NaN codes. \n",
    "Because there is no corresponding value in the reference field, we can simply drop them from our process.\n",
    "\n",
    "Finally, we truncate the floating decimals by converting the field into an integer.\n",
    "'''\n",
    "\n",
    "main_df = main_df.dropna(subset=['reference_id'])\n",
    "main_df['reference_id'] = main_df['reference_id'].astype(int)\n",
    "main_df['accountno'] = main_df['accountno'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83600927-3c48-4aec-8411-2779a4492ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As specified by the dev team, we need to make sure the character level isn't greater than 50 and 255, respectively, for ref_id and reference content.\n",
    "'''\n",
    "\n",
    "max_length = main_df['reference'].astype(str).apply(len).max()\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44eb0856-295e-4474-b46c-2b8e016ae200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I'd like to create a unique mapping list for later reference.\n",
    "\n",
    "# Create a new DataFrame with unique combinations\n",
    "unique_df = main_df.drop_duplicates(subset=['accountno', 'name', 'id']).copy()\n",
    "unique_df.drop(columns=['reference_id', 'reference'], inplace=True)\n",
    "\n",
    "# Sort the DataFrame based on the 'id' field\n",
    "unique_df_sorted = unique_df.sort_values(by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fafc9c11-eb99-4672-b5c9-d65d5fdabe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rename the columns to the format specified by engineering.\n",
    "'''\n",
    "\n",
    "main_df.rename(columns={'reference_id': 'Reference_name', 'reference': 'Reference_description'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadb1326-e0b4-4b2c-ae91-34d80933b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of main_df with the dropped columns #\n",
    "drop_df = main_df.drop(columns=['name', 'accountno'])\n",
    "drop_df['Reference_name'] = drop_df['Reference_description']\n",
    "drop_df['Reference_description'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eaba70-93f7-4769-ad78-f3f9e3bdbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The max lengths are 5 and 40, respectively. I think now we can 1) drop fields we don't need and 2) split out the parent file into child files for \n",
    "Engineering to process.\n",
    "'''\n",
    "\n",
    "# Write the function to split and save the CSVs #\n",
    "def split_and_save_csv_by_portal_id(df, output_directory):\n",
    "    \n",
    "    # Create a directory to save the CSV files if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Get unique portal IDs\n",
    "    unique_portal_id = drop_df['id'].unique()\n",
    "\n",
    "    # Iterate through unique portal IDs and save corresponding CSV files\n",
    "    for portal_id in unique_portal_id:\n",
    "        # Create a DataFrame with records matching the current portal ID\n",
    "        filtered_df = drop_df[drop_df['id'] == portal_id]\n",
    "        \n",
    "        # Generate the CSV file name based on the portal ID; use [0] value access to grab just the name     \n",
    "        csv_filename = os.path.join(output_directory, f\"cid_reference_{portal_id}.csv\")\n",
    "\n",
    "        \n",
    "        # Save the filtered DataFrame to a CSV file\n",
    "        filtered_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Saved CSV for client {portal_id} to {csv_filename}\")\n",
    "\n",
    "# Usage example\n",
    "output_dir = 'C:/Users/Desktop/Work/Platform Migration/Data Cleaning/Reference CSVs'  # Specify the output directory\n",
    "split_and_save_csv_by_portal_id(main_df, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16155298-5bed-4156-ab17-84cae5c7f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we just need to drop the unneeded columns from our drop_df file.\n",
    "'''\n",
    "\n",
    "directory = 'C:/Users/Desktop/Work/Platform Migration/Data Cleaning/Reference CSVs'\n",
    "\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate through each x in our y (each CSV file in our list); explicitly open each file, select only every column past the first two, and then save \n",
    "# them to the original file location.\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    csv_df = csv_df.drop(['id'], axis=1)\n",
    "    csv_df.to_csv(file_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
