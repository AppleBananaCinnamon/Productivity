{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f1f6c6-0a3d-4a26-89ba-2c18eab36075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import everything ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8203403d-3534-4612-86db-b3e67337f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2a6d67-c71b-4461-b184-022f0a15007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21e6f9e5-8c98-47b7-bf77-e94956bcad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import our Main file and the master SCP list from Mehmet ##\n",
    "main_df = pd.read_csv('C:/Users/Desktop/111023_All MSN Clients (excl. those with portal IDs).csv', low_memory = False)\n",
    "scp_df = df = pd.read_excel('C:/Users/Desktop/Work/Platform Migration/Data Cleaning/State, Country, Phone Codes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2867ce9a-a427-4ae6-9c1c-35a8cb6cd487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE INSTRUCTIONS, CREATE DATE, ACCOUNT NO.; ADD IN RESIDENTIAL, AND LOADING_DOCK ##\n",
    "\n",
    "main_df.drop(columns=['instructions', 'create_date'], inplace=True)\n",
    "main_df['Residential_Address'] = 'No'\n",
    "main_df['Loading_Dock'] = 'No'\n",
    "main_df['Group_Name'] = ''\n",
    "main_df.rename(columns={'id': 'Portal_ID'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d09aa448-5ce3-434c-ac19-ca76df30e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EMAIL ADDRESS CLEAN ##\n",
    "def process_email(email_address):\n",
    "    if pd.isna(email_address): \n",
    "        return ''\n",
    "\n",
    "    email_address = str(email_address)\n",
    "    \n",
    "    # Remove white space from the email address\n",
    "    email_address = email_address.strip()\n",
    "    \n",
    "    # Take only the first email address in contact email\n",
    "    email_address = email_address.split(',')[0]\n",
    "    email_address = email_address.split(';')[0]\n",
    "    email_address = email_address.split('\\n')[0]\n",
    "    email_address = email_address.split('\\r')[0]\n",
    "    \n",
    "    return email_address\n",
    "\n",
    "main_df['Contact_Email'] = main_df['Contact_Email'].apply(process_email)\n",
    "\n",
    "'''\n",
    "Something to learn: you originally wrote \"email_address = email_address.str.strip()\" because you were borrowing from old syntax \n",
    "\"df['Column'] = df['Column'].str.strip()\". The difference is the 'email_address' is being treated as a single string, the df['email'] is a Series. \n",
    "So Python understands the df['email'] because its being told to treat the values in the Series as a string.\n",
    "But the second it doesn't understand because there's no array for it to operate on because 'email_address' might not be a Series.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e37ef6d4-03a0-453f-b8a8-714184a3bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONTACT NAME BLANK HANDLING ##\n",
    "\n",
    "def blank_name(row):\n",
    "    contact_name = str(row['Contact_Name'])\n",
    "    company_name = str(row['Company_Name'])\n",
    "\n",
    "    if contact_name == 'nan' and company_name != 'nan':\n",
    "        contact_name = company_name\n",
    "    else: \n",
    "        contact_name\n",
    "\n",
    "    return contact_name\n",
    "\n",
    "main_df['Contact_Name'] = main_df.apply(blank_name, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceb2ecf1-a3d6-43e1-878a-713c29b60240",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDRESS CONCATENATION ##\n",
    "\n",
    "'''\n",
    "Pandas makes us specify 'row' here because we're creating a custom function with the if statments, as opposed to the email address clean which \n",
    "only uses built-in functions like strip and RegEx. \n",
    "'''\n",
    "def address_concat(row):\n",
    "    add2 = str(row['Address_Line_2']).strip()\n",
    "    add3 = str(row['address3']).strip()\n",
    "    add4 = str(row['address4']).strip()\n",
    "\n",
    "    # Replace 'nan' with actual NaN (missing) values; something about stringifying these columns forces the value 'NaN' to the string 'nan'\n",
    "    add2 = None if add2 == 'nan' else add2\n",
    "    add3 = None if add3 == 'nan' else add3\n",
    "    add4 = None if add4 == 'nan' else add4\n",
    "\n",
    "    # Initialize an empty result string\n",
    "    result = ''\n",
    "\n",
    "    # Check conditions and concatenate the address lines\n",
    "    if add2:\n",
    "        result += add2\n",
    "    if add3:\n",
    "        if result:\n",
    "            result += ', ' + add3\n",
    "        else:\n",
    "            result += add3\n",
    "    if add4:\n",
    "        if result:\n",
    "            result += ', ' + add4\n",
    "        else:\n",
    "            result += add4\n",
    "\n",
    "    return result\n",
    "\n",
    "# Apply the address_concat function to overwrite the 'Address_Line_2' column\n",
    "main_df['Address_Line_2'] = main_df.apply(address_concat, axis=1)\n",
    "\n",
    "# Drop the unnecessary columns \n",
    "main_df.drop(columns=['address3', 'address4'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d9e0005-87c5-4eb1-9898-d45c8b62affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PHONE NUMBER CLEAN ##\n",
    "def remove_non_numeric_chars(phone_number):\n",
    "    if pd.isna(phone_number):\n",
    "        return ''\n",
    "        \n",
    "    phone_number = str(phone_number)\n",
    "    phone_number = phone_number.strip()\n",
    "    \n",
    "    chars_to_remove = ['-', '(', ')', ' ', '+', '.']\n",
    "    for char in chars_to_remove:\n",
    "        phone_number = phone_number.replace(char, '')\n",
    "    \n",
    "    # Remove alphabetic characters using regex\n",
    "    phone_number = re.sub(r\"[a-zA-Z]\", \"\", phone_number)\n",
    "    \n",
    "    return phone_number\n",
    "\n",
    "main_df['Phone_Number'] = main_df['Phone_Number'].apply(remove_non_numeric_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76aade54-81cb-4bc6-bbd6-dea36b6a9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INTERNATIONAL DIAL CODE CLEAN ##\n",
    "'''\n",
    "First, sanitize the Dial field in our scp file so it can match the phone numbers in main\n",
    "'''\n",
    "\n",
    "# Create a copy of Dial so we can come back to it later.\n",
    "scp_df['Prefix'] = scp_df['Dial']\n",
    "\n",
    "def process_dial_code(dial_code):\n",
    "    dial_code = str(dial_code)\n",
    "    \n",
    "    # Clean the string\n",
    "    dial_code = dial_code.split(',')[0]\n",
    "    dial_code = re.sub(r\"[^0-9]\", \"\", dial_code)\n",
    "    dial_code = dial_code.strip()\n",
    "\n",
    "    return dial_code\n",
    "\n",
    "# Apply the processing function to the 'Dial' column in scp_codes\n",
    "scp_df['Dial'] = scp_df['Dial'].apply(process_dial_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e97aec47-3dcb-4510-a9bb-a562c074fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched country codes:\n",
      "Series([], Name: Country_Code, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Second, because we're evaluating Series, we need to use a mapping. \n",
    "This will create a dictionary of value pairs between country names and codes, where name is the key and codes is the value\n",
    "'''\n",
    "country_mapping = dict(zip(scp_df['country_name'].str.upper(), scp_df['country_code'].str.upper()))\n",
    "\n",
    "# Clean and strip all the white space in Country Code\n",
    "main_df['Country_Code'] = main_df['Country_Code'].str.strip()\n",
    "main_df['Country_Code'] = main_df['Country_Code'].str.upper()\n",
    "\n",
    "'''\n",
    "For all UK's, change to GB to be consistent with master list. The reason \"for row in main_df['Country_Code']... if row = 'UK'...\" doesn't work \n",
    "is because you would only be modifying the value at the local level; this change wouldn't propagate back to the original DataFrame, because it \n",
    "doesn't know how to access the Series directly. To fix this, we use .loc + the index to tell Python which row to modify. \n",
    "'''\n",
    "main_df.loc[main_df['Country_Code'] == 'UK', 'Country_Code'] = 'UNITED KINGDOM'\n",
    "main_df.loc[main_df['Country_Code'] == 'GREAT BRITAIN', 'Country_Code'] = 'UNITED KINGDOM'\n",
    "main_df.loc[main_df['Country_Code'] == 'HONG KONG', 'Country_Code'] = 'HONG KONG SAR CHINA'\n",
    "main_df.loc[main_df['Country_Code'] == 'CZECH REPUBLIC', 'Country_Code'] = 'CZECHIA'\n",
    "main_df.loc[main_df['Country_Code'] == 'Ie', 'Country_Code'] = 'IRELAND'\n",
    "main_df.loc[main_df['Country_Code'] == 'SP', 'Country_Code'] = 'SPAIN'\n",
    "main_df.loc[main_df['Country_Code'] == 'SW', 'Country_Code'] = 'SWITZERLAND'\n",
    "main_df.loc[main_df['Country_Code'] == 'RUSSIAN FEDERATION', 'Country_Code'] = 'RUSSIA'\n",
    "main_df.loc[main_df['Country_Code'] == 'FRANCE (EUROPEAN TERRITORY)', 'Country_Code'] = 'FRANCE'\n",
    "main_df.loc[main_df['Country_Code'] == 'US VIRGIN ISLANDS', 'Country_Code'] = 'U.S. VIRGIN ISLANDS'\n",
    "main_df.loc[main_df['Country_Code'] == 'SWAZILAND', 'Country_Code'] = 'ESWATINI'\n",
    "main_df.loc[main_df['Country_Code'] == 'VATICAN CITY STATE', 'Country_Code'] = 'VATICAN CITY'\n",
    "main_df.loc[main_df['Country_Code'] == 'ON', 'Country_Code'] = 'CANADA'\n",
    "main_df.loc[main_df['Country_Code'] == 'QC', 'Country_Code'] = 'CANADA'\n",
    "main_df.loc[main_df['City'] == 'Curacao', 'Country_Code'] = 'CURAÇAO'\n",
    "main_df.loc[main_df['City'] == 'Willemstad', 'Country_Code'] = 'CURAÇAO'\n",
    "main_df.loc[main_df['City'] == 'St. Maarten', 'Country_Code'] = 'SINT MAARTEN'\n",
    "main_df.loc[main_df['City'] == 'Philipsburg', 'Country_Code'] = 'SINT MAARTEN'\n",
    "main_df.loc[main_df['City'] == 'SINT MAARTEN', 'Country_Code'] = 'SINT MAARTEN'\n",
    "main_df.loc[main_df['City'] == 'SIMPSON BAY', 'Country_Code'] = 'SINT MAARTEN'\n",
    "main_df.loc[main_df['Country_Code'] == 'NJ', 'Country_Code'] = 'UNITED STATES'\n",
    "main_df.loc[main_df['Country_Code'] == 'SF', 'Country_Code'] = 'UNITED KINGDOM'\n",
    "main_df.loc[main_df['Country_Code'] == 'NB', 'Country_Code'] = 'UNITED KINGDOM'\n",
    "main_df.loc[main_df['Country_Code'] == 'FX', 'Country_Code'] = 'FRANCE'\n",
    "main_df.loc[main_df['Country_Code'] == 'LX', 'Country_Code'] = 'LUXEMBOURG'\n",
    "main_df.loc[main_df['Country_Code'] == 'KO', 'Country_Code'] = 'SOUTH KOREA'\n",
    "main_df.loc[main_df['Country_Code'] == 'BC', 'Country_Code'] = 'CANADA'\n",
    "main_df.loc[main_df['Country_Code'] == 'EN', 'Country_Code'] = 'UNITED KINGDOM'\n",
    "main_df.loc[main_df['Country_Code'] == 'HU', 'Country_Code'] = 'HUNGARY'\n",
    "main_df.loc[main_df['Country_Code'] == 'NT', 'Country_Code'] = 'NETHERLANDS'\n",
    "main_df.loc[main_df['Country_Code'] == 'DD', 'Country_Code'] = 'DENMARK'\n",
    "main_df.loc[main_df['Country_Code'] == 'AB', 'Country_Code'] = 'CANADA'\n",
    "main_df.loc[main_df['Country_Code'] == 'HI', 'Country_Code'] = 'ARMENIA'\n",
    "main_df.loc[main_df['Country_Code'] == 'AN', 'Country_Code'] = 'ST. MARTIN'\n",
    "main_df.loc[main_df['Country_Code'] == 'TA', 'Country_Code'] = 'FRANCE'\n",
    "main_df.loc[main_df['Country_Code'] == 'PO', 'Country_Code'] = 'POLAND'\n",
    "main_df.loc[main_df['City'] == 'Zagreb', 'Country_Code'] = 'CROATIA'\n",
    "main_df.loc[main_df['Country_Code'] == 'HUNGARY', 'Country_Code'] = 'HU'\n",
    "main_df.loc[main_df['Country_Code'] == 'HG', 'Country_Code'] = 'HUNGARY'\n",
    "main_df.loc[main_df['Country_Code'] == 'BOSNIA-HERZEGOVINA', 'Country_Code'] = 'BOSNIA & HERZEGOVINA'\n",
    "main_df.loc[main_df['Country_Code'] == 'SLOVAK REPUBLIC', 'Country_Code'] = 'SLOVAKIA'\n",
    "main_df.loc[main_df['Country_Code'] == '-1', 'Country_Code'] = 'CANADA'\n",
    "main_df.loc[main_df['Country_Code'] == 'C', 'Country_Code'] = 'CANADA'\n",
    "\n",
    "'''\n",
    "After that, we want to use this mapping to replace values in our main df that are not using country code. For any unmatched value, they will not \n",
    "be changed (e.g., 'UK' originally didn't map to either 'GB' or 'United Kingdom', so we caught it with our handling. \n",
    "Note: it doesn't matter if the value in 'Country_Code' is the key (i.e., DE) or the value (Denmark). All that matters is the value the user has \n",
    "entered exists in some way in our new dictionary country_mapping.\n",
    "'''\n",
    "main_df['Country_Code'] = main_df['Country_Code'].where(main_df['Country_Code'] == main_df['Country_Code'].map(country_mapping), \n",
    "                                              main_df['Country_Code'].replace(country_mapping))\n",
    "\n",
    "# Finally, as a contingency, let's print any unmatched values.\n",
    "unmatched_mask = ~main_df['Country_Code'].isin(country_mapping.values())\n",
    "unmatched_country_codes = main_df.loc[unmatched_mask, 'Country_Code']\n",
    "print(\"Unmatched country codes:\")\n",
    "print(unmatched_country_codes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c46976a7-071e-4e16-b8b7-ef80711270c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Finally, run through each row in our main_df; check if there's a match on country code, then strip out everything up until the prefix.\n",
    "'''\n",
    "\n",
    "# Create a new, empty column called 'Int. Ext'\n",
    "main_df['Phone_Country_Code'] = None\n",
    "def process_intcode():\n",
    "    # Iterrows has two parameters; index (which is the current row's position in the df) and row (which is a Series that represents all the data within that row)\n",
    "    for index, row in main_df.iterrows():\n",
    "        country_code = row['Country_Code'] # We're accessing the column 'Country_Code' within the current row array we're iterating through\n",
    "       \n",
    "        # Check if there's a matching dial code for the country code;\n",
    "        dial_row = scp_df.loc[scp_df['country_code'] == country_code] \n",
    "        '''\n",
    "        Before the .loc; we're looking within the selected column 'country_code' within the dataframe scp and asking if - for a given row in our \n",
    "        iterated df - country code == country code. If it is, then for that iterated row, the result is an array / dataframe that would literally \n",
    "        print 'True' or 'False' for every element in the scp dataframe. The .loc is a clever way of taking the results of this Boolean mask \n",
    "        (i.e., the T and F's) and giving us rows within scp that have a country code that match the country code of our main df. This is because \n",
    "        .loc filters rows based on a condition; the condition here is where cc == cc. So the result is, in the context of our for loop, \n",
    "        'go grab every column from scp where cc == cc; then filter the resulting dataframe and show me just the rows that meet this condition.\n",
    "        '''\n",
    "        # Small addition; if the Phone_Number is blank or NaN, leave as blank\n",
    "        if pd.isna(row['Phone_Number']) or row['Phone_Number'] == '' or row['Phone_Number'] == 'nan':\n",
    "            main_df.at[index, 'Phone_Number'] = ''\n",
    "\n",
    "        \n",
    "        if not dial_row.empty:\n",
    "            dial_code = dial_row['Dial'].values[0]\n",
    "\n",
    "            # Check if the 'Phone_Number' starts with the dial code\n",
    "            if row['Phone_Number'].startswith(dial_code):\n",
    "                # Remove the dial code from the 'Phone_Number'\n",
    "                main_df.at[index, 'Phone_Number'] = row['Phone_Number'][len(dial_code):]\n",
    "\n",
    "        # Finally, add back in the UNFORMATTED intn'l dial code prefix for every row in our main_df\n",
    "        condition = scp_df['country_code'] == country_code\n",
    "        if condition.any():\n",
    "            main_df.at[index, 'Phone_Country_Code'] = scp_df.loc[condition, 'Prefix'].values[0]\n",
    "\n",
    "# Call the function to modify df['Phone_Number']\n",
    "process_intcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dee425d-85c9-4f7d-bdac-cb45c406c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STATE CODE CLEANING ##\n",
    "\n",
    "def process_state_code(state_code):\n",
    "    # Remove non-alphabetic characters\n",
    "    if pd.notna(state_code):\n",
    "        state_code = str(state_code).strip()\n",
    "        state_code = re.sub(r\"[^a-zA-Z]\", \"\", state_code)\n",
    "    else:\n",
    "        state_code = ''\n",
    "\n",
    "    # Ensure that the state_code is blank on receipt\n",
    "    if state_code == '' or state_code.lower() == 'nan':\n",
    "        state_code = ''\n",
    "\n",
    "    # Capitalize the state code\n",
    "    state_code = state_code.upper()\n",
    "\n",
    "    return state_code\n",
    "\n",
    "# Apply the processing function to the 'State_Code' column\n",
    "main_df['State_Code'] = main_df['State_Code'].apply(process_state_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "087aa97a-84d9-496d-b116-132fedee6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROVINCE CODE CLEANING ##\n",
    "\n",
    "'''\n",
    "If the province field(from our main df) matches anything in our 'state name' field (from our scp df), then replace the value with what's in \n",
    "'state code' (from our scp df)\n",
    "'''\n",
    "def process_province(province):\n",
    "    # Remove non-alphabetic characters\n",
    "    if pd.notna(province) and isinstance(province, str):\n",
    "        province = str(province).strip()\n",
    "        province = re.sub(r\"[^a-zA-Z]\", \"\", province)\n",
    "    else:\n",
    "        province = ''\n",
    "\n",
    "    # Split the input string based on where a lowercase letter is followed by an uppercase letter (e.g., 'NorthCarolina' should = 'North Carolina')\n",
    "    province = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", province)\n",
    "\n",
    "    # Check if province matches 'state_name' in scp_codes\n",
    "    mask = scp_df['state_name'].str.lower() == province.lower()\n",
    "    if mask.any():\n",
    "        # Replace with corresponding 'state_code'\n",
    "        province = scp_df.loc[mask, 'state_code'].values[0]\n",
    "        \n",
    "    return province\n",
    "\n",
    "# Apply the processing function to the 'province' column\n",
    "main_df['province'] = main_df['province'].apply(process_province)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fab93220-eb60-4908-b7e4-4a2fc604f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ZIP CODE CLEANING ##\n",
    "\n",
    "def process_zip_code(zip_code):\n",
    "    # Remove non-numeric characters\n",
    "    if pd.notna(zip_code):\n",
    "        zip_code = str(zip_code).strip()\n",
    "        zip_code = re.sub(r\"[^0-9]\", \"\", zip_code)\n",
    "    else:\n",
    "        zip_code = ''\n",
    "\n",
    "    # Replace blanks and nan values\n",
    "    if zip_code == '' or zip_code.lower() == 'nan':\n",
    "        zip_code = ''\n",
    "\n",
    "    return zip_code\n",
    "\n",
    "# Apply the processing function to the 'Zip_Code' column\n",
    "main_df['Zip_Code'] = main_df['Zip_Code'].apply(process_zip_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4347b0f0-dfea-4806-8fb0-dcf0b19b0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "## POSTAL CODE CLEANING ##\n",
    "\n",
    "def process_postal_code(postal_code):\n",
    "    # Remove non-alphabetic characters\n",
    "    if pd.notna(postal_code):\n",
    "        postal_code = str(postal_code).strip()\n",
    "        postal_code = re.sub(r\"[^a-zA-Z0-9]\", \"\", postal_code)\n",
    "    else:\n",
    "        postal_code = ''\n",
    "\n",
    "    # Replace blanks and nan values\n",
    "    if postal_code == '' or postal_code.lower() == 'nan':\n",
    "        postal_code = ''\n",
    "\n",
    "    return postal_code\n",
    "\n",
    "# Apply the processing function to the 'postalcode' column\n",
    "main_df['postalcode'] = main_df['postalcode'].apply(process_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5925e-e339-487d-b5c4-8264bb348cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Check for float types in province; for some reason, we usually have to run the 'province' clean twice.\n",
    "'''\n",
    "\n",
    "float_province_values = main_df.loc[main_df['province'].apply(lambda x: isinstance(x, float)), 'province']\n",
    "\n",
    "# Print all values\n",
    "print(float_province_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f53ab884-5b69-484f-a087-8607f979fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE NEW STATE/PROVINCE + ZIP/POSTAL FIELDS ##\n",
    "# Create a new field in df that has EITHER -- not both -- of state code / province code, and zip code / postal code\n",
    "\n",
    "def merge_state_province(row):\n",
    "    if row['State_Code'] == '' or row['State_Code'].lower() == 'nan':\n",
    "        if row['province'] == '' or row['province'].lower() == 'nan':\n",
    "            return ''\n",
    "        else:\n",
    "            return row['province']\n",
    "    else:\n",
    "        return row['State_Code']\n",
    "        \n",
    "def merge_zip_postal(row):\n",
    "    if row['Zip_Code'] == '' or row['Zip_Code'] == 'nan':\n",
    "        if row['postalcode'] == '' or row['postalcode'] == 'nan':\n",
    "            return ''\n",
    "        else:\n",
    "            return row['postalcode']\n",
    "    else:\n",
    "        return row['Zip_Code']\n",
    "\n",
    "# Apply the function row-wise to create the 'New_State_Province' column. If I stipulated 'axis = 0' here, it would apply the formatting column-wise.\n",
    "main_df['New_State_Province'] = main_df.apply(merge_state_province, axis=1)\n",
    "main_df['New_Zip_Postal'] = main_df.apply(merge_zip_postal, axis=1)\n",
    "\n",
    "# Assign the new values to the old columns\n",
    "main_df['State_Code'] = main_df['New_State_Province'] \n",
    "main_df['Zip_Code'] = main_df['New_Zip_Postal']\n",
    "\n",
    "# Drop the redundant columns\n",
    "main_df.drop(columns=['New_State_Province', 'New_Zip_Postal', 'postalcode', 'province'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e6735a2-2f3e-41fd-9af0-0df0e58690b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTENSION NUMBER CLEANING ##\n",
    "def process_ext_code(ext_code):\n",
    "    # Remove non-alphabetic characters\n",
    "    ext_code = str(ext_code)\n",
    "    ext_code = re.sub(r\"[^0-9]\", \"\", ext_code)\n",
    "\n",
    "    # Remove white space\n",
    "    ext_code = ext_code.strip()\n",
    "\n",
    "    # Ensure that the ext_code is blank on receipt\n",
    "    if ext_code == '':\n",
    "        ext_code = ''\n",
    "\n",
    "    return ext_code\n",
    "\n",
    "# Apply the processing function \n",
    "main_df['Phone_Number_Ext'] = main_df['Phone_Number_Ext'].apply(process_ext_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1f14a2d-39c1-482e-b526-fcb103a2bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REORDER THE COLUMNS IN THE WAY SPECIFIED BY THE DEV TEAM ##\n",
    "\n",
    "current_order = main_df.columns.tolist()\n",
    "desired_order = ['Portal_ID', 'accountno', 'Client', 'Group_Name', 'City', 'Company_Name', 'Contact_Email', 'Contact_Name', 'Phone_Number', 'Country_Code', 'State_Code', 'Zip_Code', 'Phone_Country_Code', 'Phone_Number_Ext', 'Street_Address', 'Address_Line_2', 'Residential_Address', 'Loading_Dock']  \n",
    "main_df = main_df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ffb3a99e-b69b-4558-a6b4-4ce2813cbf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Contact_Name' contains the illegal character.\n",
      "Column 'Street_Address' contains the illegal character.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Trying to export main_df to Excel is giving us an 'illegal character' error; we need to ID which column contains the error.\n",
    "'''\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "illegal_char = '\\x1f'\n",
    "\n",
    "# Iterate through each column\n",
    "for column in main_df.columns:\n",
    "    # Check if the illegal character is present in any cell of the column\n",
    "    has_illegal_char = any(main_df[column].astype(str).str.contains(illegal_char))\n",
    "\n",
    "    # If the illegal character is present, print the column name\n",
    "    if has_illegal_char:\n",
    "        print(f\"Column '{column}' contains the illegal character.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0e63f-56d5-46bd-98a0-4c59f90e2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As a redundancy, I'd like to ID the exact row where the issue occurs.\n",
    "'''\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "problematic_rows = main_df[main_df['Company_Name'] == 'Sam\u0019s Club']\n",
    "print(problematic_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4ebf6-1ae7-4f71-ba38-0521dde5676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We're still identifying areas where the field isn't capturing all illegal characters. Let's try to print all the rows where problematic characters \n",
    "exist.\n",
    "'''\n",
    "\n",
    "column_name = 'Company_Name'\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, value in main_df[column_name].items():\n",
    "    # Check if the value contains non-printable characters\n",
    "    if any(not char.isprintable() for char in str(value)):\n",
    "        print(f\"Non-printable characters found in '{column_name}' at index {index}: {repr(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4bf95b4a-6a42-48ab-9564-4794a5bec30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that we've ID'd which column and which row have the illegal character, let's just replace them with a blank so we can export \n",
    "our main_df to Excel and be done. Because its such a big file, let's make a new df to ensure if we make a mistake, we can revert back.\n",
    "'''\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "main_1_df = main_df.copy()\n",
    "\n",
    "# Replace non-printable characters in 'Company_Name' with an empty string\n",
    "main_1_df['Company_Name'] = main_1_df['Company_Name'].apply(lambda x: ''.join(char for char in str(x) if char.isprintable()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbf4503d-2772-4c2e-a9cf-6a8bf3437460",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's check to make sure there are no more unprintable characters, if this runs smoothly, we're set.\n",
    "'''\n",
    "column_name = 'Company_Name'\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, value in main_1_df[column_name].items():\n",
    "    # Check if the value contains non-printable characters\n",
    "    if any(not char.isprintable() for char in str(value)):\n",
    "        print(f\"Non-printable characters found in '{column_name}' at index {index}: {repr(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb3774d0-eaf3-4a92-a2c3-d42cbe33ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_1_df.to_csv('111523_Cleaned MSN Clients.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a69fd-9aae-4496-b034-17964efdd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT THE MAIN FILE INTO SEPARATE CHILD CSVS \n",
    "\n",
    "def split_and_save_csv_by_portal_id(df, output_directory):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create a directory to save the CSV files if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Get unique portal IDs\n",
    "    unique_portal_id = df['Portal_ID'].unique()\n",
    "\n",
    "    # Iterate through unique portal IDs and save corresponding CSV files\n",
    "    for portal_id in unique_portal_id:\n",
    "        # Create a DataFrame with records matching the current portal ID\n",
    "        filtered_df = df[df['Portal_ID'] == portal_id]\n",
    "        unique_client_name = filtered_df['Client'].unique()\n",
    "        \n",
    "        # Generate the CSV file name based on the portal ID; use [0] value access to grab just the name     \n",
    "        csv_filename = os.path.join(output_directory, f\"cid_{portal_id}.csv\")\n",
    "\n",
    "        \n",
    "        # Save the filtered DataFrame to a CSV file\n",
    "        filtered_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Saved CSV for client {portal_id}_{unique_client_name[0]} to {csv_filename}\")\n",
    "\n",
    "# Usage example\n",
    "output_dir = 'C:/Users/Desktop/Work/Coding/CSVs to Upload'  # Specify the output directory\n",
    "split_and_save_csv_by_portal_id(main_df, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de292be9-161b-4835-bd78-f894854f0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH OUR CHILD FILES CREATED, NOW, DELETE THE FIRST TWO COLUMNS FROM EVERY CSV  ##\n",
    "\n",
    "directory = 'C:/Users/Desktop/Work/Coding/CSVs to Upload'\n",
    "\n",
    "'''\n",
    "This is called 'list comprehension, and it is a more Pythonic way of creating and assigning a new list. The logic below is the exact same logic as:\n",
    "csv_file_list = []\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.csv'):\n",
    "         csv_file_list.append(file)\n",
    "Effectively, we're executing the separate list creation and assignment in one line of code as opposed to 4\n",
    "This is creating a working list of all our CSV files at a specified location and assigning them the variable 'f'.\n",
    "Note, while we could've named 'f' anything, we would get a Name Error if our preceing and succeeding variables aren't the same, and that's purely\n",
    "because list comprehension by nature both creates and assigns our variables in one line. If we had named the succeeding variable 'e', for example,\n",
    "we would need to define what 'e' is before our list comprehension executes.\n",
    "'''\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate through each x in our y (each CSV file in our list); explicitly open each file, select only every column past the first two, and then save \n",
    "# them to the original file location.\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    csv_df = csv_df.iloc[:, 2:]\n",
    "    csv_df.to_csv(file_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9919b8c-9c69-4647-8105-53499061795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE A ZIPPED FILE OF ALL THE CSV'S ##\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def zip_directory(source_folder, zip_filename):\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(source_folder):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, source_folder)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "source_folder = 'CSVs to Upload'\n",
    "zip_filename = 'my_files.zip'\n",
    "zip_directory(source_folder, zip_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb20de-785b-478e-b029-25dfe3ae40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of scratch, but this is useful for filtering the column of an entire df on a single condition.\n",
    "main_df.loc[main_df['Country_Code'] != 'US'][:2]\n",
    "main_df.loc[main_df['Country_Code'].isna()][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "d94209c9-63de-4f03-bdf3-2dd0cd2725ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = pd.read_excel('C:/Users/Desktop/Work/Platform Migration/Data Cleaning/Migration Update 10-17-23.xlsx', sheet_name = 'Clients to Migrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "cc0d7b35-e816-47e7-92c4-05972e52a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'4532', '4480', '78989', '79132', '7712', '4124', '7081', '7059', '18011', '86929', '80965', '30660', '74821', '7558', '71223', '71132', '73391', '6887', '72914', '7164', '4137', '7597', '7542', '7646', '71006', '1803', '83396', '80460', '7705', '73881', '7731', '82222', '79978', '72911', '85995', '7771', '5966', '7617', '71789', '5562', '7501', '7100', '5667', '7790', '6799', '7691', '75341', '9701', '7539', '3569', '7614', '75923', '75821', '4756', '5130', '5168', '5192', '5226', '5252', '5510', '5573', '5776', '5960', '6787', '6882', '7038', '7078', '7080', '7126', '7146', '7154', '7158', '7221', '7227', '7233', '7292', '7295', '7344', '7424', '7440', '7471', '7519', '7550', '7572', '7592', '7598', '7615', '7623', '7624', '7632', '7635', '7671', '7681', '7719', '7759', '7760', '7770', '7773', '7774', '7854', '7906', '7919', '7926', '8560', '9390', '9880', '18003', '18018', '30005', '30750', '31123', '40060', '41003', '41236', '70006', '70013', '70032', '70054', '70063', '70106', '70137', '71003', '71238', '71277', '71752', '72221', '72291', '73832', '75555', '75656', '75922', '75929', '75991', '77772', '78732', '79122', '79542', '79921', '79998', '80066', '80601', '81515', '82440', '83025', '83211', '83411', '85225', '85340', '85435', '86530', '86575', '86622', '86665', '86860', '87090', '87311', '87745', '89006', '90048', '90149', '90156', '90167', '90169', '90184', '1015', '2255', '3405', '3777', '4142', '4326',\n"
     ]
    }
   ],
   "source": [
    "def q_list(account):\n",
    "    acct_no = str(account).strip()\n",
    "    return f\"'{acct_no}', \"\n",
    "\n",
    "# Apply the function to each element in the 'Account Number' column\n",
    "concatenated_values = query_list['Account Number'].apply(q_list)\n",
    "\n",
    "# Join the results into a single string\n",
    "concatenated_values = concatenated_values.str.cat()\n",
    "\n",
    "# Remove the trailing comma\n",
    "concatenated_values = concatenated_values[:-1]\n",
    "\n",
    "# Print the concatenated values or return them as needed\n",
    "print(concatenated_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fc0c7-dbde-4b9d-b473-5ae99ec9a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Think about the way Pandas ingests Series in this way: it's going to treat each element of the field separately, so your named parameter,\n",
    "in this case, \"client_name\", is the thing it's going to iterate through. The '.apply' method takes a function as a parameter, and it's going to apply \n",
    "that function to every element within the field you specify (in this case, \"Client Name\"). Concatenated Comma then takes the output of that modified \n",
    "list, and concatenates all the elements into one line. We then are printing that list and removing the very last comma with [:-1] (this is clever; b/c\n",
    "we've now created a very long list of many, many characters, we can use string slicing to capture everything (the [:...]) up until the penultimate \n",
    "positioned character (the -1). \n",
    "'''\n",
    "\n",
    "def format_list(client_name):\n",
    "    client_name = str(client_name).strip()\n",
    "    return f\"'{client_name}',\"\n",
    "\n",
    "comma_list = migration_list['Client Name'].apply(format_list)\n",
    "concat_comma = comma_list.str.cat()\n",
    "concat_comma = concat_comma[:-1]\n",
    "print(concat_comma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "00bc1ed2-17f9-4964-9f7e-7000784c5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "migration_list = pd.read_excel('C:/Users/Desktop/Work/Platform Migration/Migration Tracker/110323_Migration Tracker.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
