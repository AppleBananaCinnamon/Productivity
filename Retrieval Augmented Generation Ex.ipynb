{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e711541-be4f-4c46-8fa1-602b04f8630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "import pprint\n",
    "import difflib\n",
    "from IPython.display import display, HTML\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "import time\n",
    "from openai import RateLimitError\n",
    "from scipy import spatial\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "\n",
    "api_key = \"...\"  # Replace with your actual environment variable name\n",
    "\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-1106\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e7bebc-0ef7-403a-adde-0a57441d4d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1741 article titles in Category:Computer science.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As I've been trying to teach myself more about computer science, I thought it would be interesting to train GPT to have more specialized knowledge\n",
    "on Computer Science using a 'Search-Ask' methodology, specifically, an embedding-based search. To accomplish this, I set out to Collect, Chunk, Embed\n",
    "and Store embeddings (using online free content from Wikipedia) to feed the database GPT will draw from to help it answer questions (i.e., Retrieval \n",
    "Augmented Generation, or RAG).\n",
    "'''\n",
    "\n",
    "#############\n",
    "#  COLLECT  #\n",
    "#############\n",
    "\n",
    "CATEGORY_TITLE = \"Category:Computer science\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category, max_depth: int\n",
    ") -> set[str]:\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "# ^note: max_depth=1 means we go one level deep in the category tree\n",
    "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4faa3e98-b771-4edd-9d1d-e22961ff3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to split Wikipedia pages into sections\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,\n",
    "    parent_titles: list[str],\n",
    "    sections_to_ignore: set[str],\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    \n",
    "    ''' \n",
    "    This is the base case of our recursive function.\n",
    "    '''\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results\n",
    "\n",
    "    '''    \n",
    "    And this is the recursive case of our function because its calling itself 'all_subsections_from_section' to complete its task. Notice, we use \n",
    "    results.extend() in this case vs. results.append(). This is because we want to unpack and add the iterable to our running list, rather than simply\n",
    "    add the entire iterable. E.g., if we want to add [(3,c), (4,d)] to our initial list of two tuples of [(1,a), (2,b)] -> .append() would create \n",
    "    [(1,a), (2,b), [(3,c), (4,d)]], which is technically now three elements: two tuples, and one list containing two tuples. .expand() would instead\n",
    "    create one list of four tuples[(1,a), (2,b), (3,c), (4,d)]. \n",
    "    '''\n",
    "\n",
    "\n",
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3298ce5b-1d57-42ff-af4c-5ae07da6cc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8708 sections in 1743 pages.\n"
     ]
    }
   ],
   "source": [
    "# split pages into sections\n",
    "# may take ~1 minute per 100 articles\n",
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1adf3b5c-d9c6-4677-a5d0-21f93c26d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 355 sections, leaving 8353 sections.\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
    "    titles, text = section\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003f115b-6ba6-451b-87a7-b3db5eda39e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eyeball network']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{Multiple issues|{{more citations needed|date=February 2017}}{{notability|1=...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Digital Humanities conference']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{Infobox Academic Conference\\n | logo =\\n | history = 1989-\\n | discipline = [[...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Digital Humanities conference', '==History==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The first joint conference was held in 1989, at the [[University of Toronto]]...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Digital Humanities conference', '== Conferences ==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{| class=\"wikitable\"\\n|-\\n!Year\\n!Location\\n!Links\\n!Observations\\n|-\\n|1990\\n|[[Univ...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Structural information theory']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{{COI|date=December 2015}}\\n'''Structural information theory''' ('''SIT''') is...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    display(ws[1][:77] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4054d34d-8d0a-4abc-8067-f63f7629c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d6a541d-304b-4484-a567-ef863de31bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Truncated string from 1812 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 2295 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1732 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1694 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1664 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1815 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1697 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1745 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1754 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1710 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1685 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1718 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1736 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1734 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1639 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1637 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1892 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1639 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1723 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1638 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1739 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1608 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1777 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1618 tokens to 1600 tokens.\n",
      "Warning: Truncated string from 1725 tokens to 1600 tokens.\n",
      "8353 Wikipedia sections split into 8538 strings.\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "#  CHUNK  #\n",
    "###########\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5dcc201-52e8-4ed9-bd10-2c04ca1d651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyeball network\n",
      "\n",
      "{{Multiple issues|{{more citations needed|date=February 2017}}{{notability|1=Neologisms|date=February 2017}}\n",
      "}}\n",
      "\n",
      "'''Eyeball network''' is a [[slang]] term used by [[Computer network|network engineers]] and architects that refers to an access network whose primary users use the [[Computer network|network]] to “look at things” (browse the Internet, read email, etc.) and consume content, as opposed to a network that may be used primarily to generate its own data, or “content networks/providers”.\n",
      "\n",
      "The term “eyeball network” is often overheard in conversations and seen in articles that discuss peering relationships between other networks, as well as [[net neutrality]] issues.\n",
      "\n",
      "An example of an eyeball network would be any given [[Internet service provider|ISP]] that provides internet connectivity to end-users – The ISP may [[Peering|peer]] with [[Google]] (which is a content provider) where the end users consume content serviced/provided by Google, in this case the ISP is just an “eyeball network” providing a means for the end user to reach Google provided actual content.\n",
      "\n",
      "However, it is to be noted that '''not''' all ISPs are eyeball networks, they can be pure [[Tier 1 network|transit providers]]. With [[Tier 2 network|Tier 2]] networks and lower, they can serve as both an eyeball network and a transit provider, depending on their business model. In the modern day ecosystem where peering is given priority, the lines are blurred between the different types of networks as ultimately any given network must be able to reach every other given network on the internet at large.\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "print(wikipedia_strings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64d873d4-13dc-465c-b877-aef714f001f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n",
      "Batch 1000 to 1999\n",
      "Batch 2000 to 2999\n",
      "Batch 3000 to 3999\n",
      "Batch 4000 to 4999\n",
      "Rate limit reached. Stopping.\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "#  EMBED  #\n",
    "###########\n",
    "\n",
    "\n",
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "BATCH_SIZE = 1000\n",
    "MAX_BATCH_START = 4999 * BATCH_SIZE  # Stopping condition\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    if batch_start >= MAX_BATCH_START:\n",
    "        print(f\"Reached batch {batch_start}. Stopping.\")\n",
    "        break  # Break out of the loop when reaching the threshold\n",
    "\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    \n",
    "    try:\n",
    "        response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "        if response and hasattr(response, 'data'):\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "    except RateLimitError as e:\n",
    "        print(\"Rate limit reached. Stopping.\")\n",
    "        break  # Stop the script if a rate limit error occurs\n",
    "\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings[:len(embeddings)], \"embedding\": embeddings})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7a67a00-a19c-4f2c-b3ac-afb1fdc8e6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eyeball network\\n\\n{{Multiple issues|{{more ci...</td>\n",
       "      <td>[-0.00909820944070816, 0.005384792108088732, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Humanities conference\\n\\n{{Infobox Aca...</td>\n",
       "      <td>[-0.023694409057497978, -0.004818083252757788,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Digital Humanities conference\\n\\n==History==\\n...</td>\n",
       "      <td>[-0.02261469140648842, -0.009407395496964455, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Digital Humanities conference\\n\\n== Conference...</td>\n",
       "      <td>[-0.01651986874639988, -0.00649561220780015, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Digital Humanities conference\\n\\n== Conference...</td>\n",
       "      <td>[-0.014532341621816158, 0.0007241961429826915,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Structural information theory\\n\\n{{COI|date=De...</td>\n",
       "      <td>[-0.016226185485720634, 0.025272049009799957, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Structural information theory\\n\\n== The simpli...</td>\n",
       "      <td>[-0.016453608870506287, 0.04184497892856598, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Structural information theory\\n\\n== Simplicity...</td>\n",
       "      <td>[0.004965457133948803, 0.01153535209596157, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Structural information theory\\n\\n== Modeling p...</td>\n",
       "      <td>[-0.01424167025834322, 0.01915259100496769, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Structural information theory\\n\\n== Visual reg...</td>\n",
       "      <td>[-0.009177683852612972, 0.013527488335967064, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Eyeball network\\n\\n{{Multiple issues|{{more ci...   \n",
       "1  Digital Humanities conference\\n\\n{{Infobox Aca...   \n",
       "2  Digital Humanities conference\\n\\n==History==\\n...   \n",
       "3  Digital Humanities conference\\n\\n== Conference...   \n",
       "4  Digital Humanities conference\\n\\n== Conference...   \n",
       "5  Structural information theory\\n\\n{{COI|date=De...   \n",
       "6  Structural information theory\\n\\n== The simpli...   \n",
       "7  Structural information theory\\n\\n== Simplicity...   \n",
       "8  Structural information theory\\n\\n== Modeling p...   \n",
       "9  Structural information theory\\n\\n== Visual reg...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.00909820944070816, 0.005384792108088732, 0...  \n",
       "1  [-0.023694409057497978, -0.004818083252757788,...  \n",
       "2  [-0.02261469140648842, -0.009407395496964455, ...  \n",
       "3  [-0.01651986874639988, -0.00649561220780015, 0...  \n",
       "4  [-0.014532341621816158, 0.0007241961429826915,...  \n",
       "5  [-0.016226185485720634, 0.025272049009799957, ...  \n",
       "6  [-0.016453608870506287, 0.04184497892856598, 0...  \n",
       "7  [0.004965457133948803, 0.01153535209596157, 0....  \n",
       "8  [-0.01424167025834322, 0.01915259100496769, 0....  \n",
       "9  [-0.009177683852612972, 0.013527488335967064, ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a7650bd-c806-49b6-b27d-8fe7f801eae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This line of code will always produce 1536 regardless of the length of text, and its how GPT assesses and assigns semantic meaning to a piece of text. \n",
    "The 1536 are the different dimensions GPT uses to assess style, tone, meaning -> semantic purpose. So in effect, what we've done is broken down many,\n",
    "many Wikipedia pages related to Comp Sci and can now assign this text to a chat completion model to see how reliably it uses the information to \n",
    "answer queries.\n",
    "'''\n",
    "\n",
    "print(len(df.loc[3, 'embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34b491ac-bfdc-4396-aaf2-0257ffcdaf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#  STORE  #\n",
    "###########\n",
    "\n",
    "\n",
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"C:/Users/jmfra/Desktop/Coding/GPT/comp_sci_embeds.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1504d63-c3e8-4d98-afa1-bae8cec5716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is now the portion that determines the distance between the query and the embedded text dimensions to approxmiate the 'relevance' of which text \n",
    "the LLM should draw from. Here, I used Kranzberg's laws of technology, which, interestingly enough, were not contained within the Wikipedia articles\n",
    "I scraped (perhaps because I stopped the code at 5000 chunks since I kept getting a rate limit error) so in this example, we can see which 5 articles \n",
    "the model believes to be the most relevant to Kranzberg's laws of technology; either way, its interesting to see how GPT handles such a query \n",
    "when it has no 'knowledge' of the event we're asking it to pull. \n",
    "'''\n",
    "\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cda7b121-1853-4b60-a0fa-8aeb7e93dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Karl Küpfmüller\\n\\n==Studies in communication theory==\\n\\nAbout 1928, he did the same analysis that [[Harry Nyquist]] did, to show that not more than 2B independent pulses per second could be put through a channel of bandwidth B.  He did this by quantifying the time-bandwidth product \\'\\'k\\'\\' of various communication signal types, and showing that \\'\\'k\\'\\' could never be less than 1/2.  From his 1931 paper (rough translation from Swedish):<ref>Karl Küpfmüller, \"Utjämningsförlopp inom Telegraf- och Telefontekniken\", (\"Transients in telegraph and telephone engineering\"), \\'\\'[[Teknisk Tidskrift]]\\'\\', no. 9 pp.153-160 and 10 pp.178-182, 1931. (Swedish) [http://runeberg.org/tektid/1931e/0157.html] [http://runeberg.org/tektid/1931e/0182.html]\\n</ref>\\n\\n:\"The time law allows comparison of the capacity of each transfer method with various known methods. On the other hand it indicates the limits that the development of technology must stay within. One interesting question for example is where the lower limit for \\'\\'k\\'\\' lies. The answer is acquired by at least one power change being needed to achieve one signal. So the frequency range must be at least so wide that the settling time becomes less than the duration of a signal, and from this comes \\'\\'k\\'\\'=1/2. So we can never get below this value, no matter how technology develops.\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bandwidth (computing)\\n\\n==Edholm\\'s law==\\n\\n{{Main|Edholm\\'s law}}\\n\\n[[Edholm\\'s law]], proposed by and named after Phil Edholm in 2004, holds that the bandwidth of [[telecommunication network]]s double every 18 months, which has proven to be true since the 1970s. The trend is evident in the cases of [[Internet]],<ref name=\"Cherry\"/> [[cellular network|cellular]] (mobile), [[Wireless LAN|wireless]] [[Local area network|LAN]] and [[Personal area network|wireless personal area networks]].<ref name=\":1\" />\\n\\nThe [[MOSFET]] (metal–oxide–semiconductor field-effect transistor) is the most important factor enabling the rapid increase in bandwidth. The MOSFET (MOS transistor) was invented by [[Mohamed M. Atalla]] and [[Dawon Kahng]] at [[Bell Labs]] in 1959, and went on to become the basic building block of modern [[telecommunications]] technology. Continuous [[MOSFET scaling]], along with various advances in MOS technology, has enabled both [[Moore\\'s law]] ([[transistor count]]s in [[integrated circuit]] chips doubling every two years) and Edholm\\'s law (communication bandwidth doubling every 18 months).<ref name=\"Jindal\"/>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tyranny of numbers\\n\\n==History==\\n\\n[[File:Computer Museum of America (32).jpg|thumb|right|The [[Cray-1]] contained 50 miles of wiring.]]\\nThe first known recorded use of the term in this context was made by the Vice President of [[Bell Labs]] in an article celebrating the 10th anniversary of the invention of the [[transistor]], for the \"Proceedings of the IRE\" (Institute of Radio Engineers), June 1958 [https://books.google.com/books?id=3BLGWnmQJ9IC&pg=PA21]. Referring to the problems many designers were having, he wrote:\\n\\n{{Quotation|For some time now, electronic man has known how \\'in principle\\' to extend greatly his visual, tactile, and mental abilities through the digital transmission and processing of all kinds of information. However, all these functions suffer from what has been called \\'the tyranny of numbers.\\' Such systems, because of their complex digital nature, require hundreds, thousands, and sometimes tens of thousands of electron devices.|Jack Morton|[http://everything2.com/e2node/The%2520Tyranny%2520of%2520Numbers The Tyranny of Numbers]}}\\n\\nAt the time, computers were typically built up from a series of \"modules\", each module containing the electronics needed to perform a single function. A complex circuit like an [[adder (electronics)|adder]] would generally require several modules working in concert. The modules were typically built on [[printed circuit boards]] of a standardized size, with a connector on one edge that allowed them to be plugged into the power and signaling lines of the machine, and were then wired to other modules using [[twisted pair]] or [[coaxial cable]].\\n\\nSince each module was relatively custom,  modules were assembled and soldered by hand or with limited automation. As a result, they suffered major reliability problems. Even a single bad component or solder joint could render the entire module inoperative. Even with properly working modules, the mass of wiring connecting them together was another source of construction and reliability problems. As computers grew in complexity, and the number of modules increased, the complexity of making a machine actually work grew more and more difficult. This was the \"tyranny of numbers\".\\n\\nIt was precisely this problem that [[Jack Kilby]] was thinking about while working at [[Texas Instruments]]. Theorizing that germanium could be used to make all common electronic components - resistors, capacitors, etc. - he set about building a single-slab component that combined the functionality of an entire module. Although successful in this goal, it was [[Robert Noyce]]\\'s silicon version and the associated fabrication techniques that make the [[integrated circuit]] (IC) truly practical.\\n\\nUnlike modules, ICs were built using [[photoetching]] techniques on an [[assembly line]], greatly reducing their cost. Although any given IC might have the same chance of working or not working as a module, they cost so little that if they didn\\'t work you simply threw it away and tried another. In fact, early IC assembly lines had failure rates around 90% or greater, which kept their prices high. The [[U.S. Air Force]] and [[NASA]] were major purchasers of early ICs, where their small size and light weight overcame any cost issues. They demanded high reliability, and the industry\\'s response not only provided the desired reliability but meant that the increased yield had the effect of driving down prices.\\n\\nICs from the early 1960s were not complex enough for general computer use, but as the complexity increased through the 1960s, practically all computers switched to IC-based designs. The result was what are today referred to as the [[history of computing hardware|third-generation computers]], which became commonplace during the early 1970s. The progeny of the integrated circuit, the [[microprocessor]], eventually superseded the use of individual ICs as well, placing the entire collection of modules onto one chip.\\n\\n[[Seymour Cray]] was particularly well known for making complex designs work in spite of the tyranny of numbers. His attention to detail and ability to fund several attempts at a working design meant that pure engineering effort could overcome the problems they faced. Yet even Cray eventually succumbed to the problem during the [[CDC 8600]] project, which eventually led to him leaving [[Control Data]].'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Large-scale Complex IT Systems\\n\\n== Key publications ==\\n\\n=== Socio-technical systems engineering ===\\n\\n*{{cite journal | last1 = Baxter | first1 = G. | last2 = Sommerville | first2 = I. | year = 2010 | title = Socio-Technical Systems: From Design Methods to Systems Engineering | doi = 10.1016/j.intcom.2010.07.003 | journal = Interacting with Computers | volume =  23| pages =  4–17| doi-access = free }}\\n*I. Sommerville (editor). [http://archive.cs.st-andrews.ac.uk/STSE-Handbook/ The Socio-technical Systems Engineering Handbook.] (2011). University of St Andrews.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relatedness=0.789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Large-scale Complex IT Systems\\n\\n== Key publications ==\\n\\n=== Predictable software systems ===\\n\\n*R. Calinescu, L. Grunske, M. Kwiatkowska, R. Mirandola, G. Tamburrelli (2011). [http://www.computer.org/portal/web/csdl/doi/10.1109/TSE.2010.92 Dynamic QoS Management and Optimisation in Service-Based Systems.] In: IEEE Transactions on Software Engineering.\\n*L. Feng, M. Kwiatkowska and D. Parker. (2011) [http://qav.cs.ox.ac.uk/papers/fase11.pdf Automated Learning of Probabilistic Assumptions for Compositional Reasoning.] Proc. 14th International Conference on Fundamental Approaches to Software Engineering (FASE'11), volume 6603 of LNCS, pages 2–17, Springer.\\n*M. Kwiatkowska. (2007) [http://qav.cs.ox.ac.uk/papers/esec-fse07.pdf Quantitative Verification: Models, Techniques and Tools.] Proc. 6th joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE), pages 449-458, ACM Press.\\n*M. Kwiatkowska, G. Norman and D. Parker. (2009) [http://qav.cs.ox.ac.uk/papers/acmper_prismperf.pdf PRISM: Probabilistic Model Checking for Performance and Reliability Analysis.] ACM SIGMETRICS Performance Evaluation Review, 36(4), pages 40–45, ACM.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examples\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"Kranzberg's laws of technology\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e15913bd-0fd8-4721-b82f-aeda603080fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this final bit, we 1) create a token budget for the model to process, 2) use that token budget to ensure we're only modestly pulling articles and \n",
    "producing text; 3) asking ChatGPT to take our embedded text and formulate a response to whichever query we pose it. \n",
    "'''\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below articles on Computer Science to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions about Computer Science.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "771de1de-e328-4d98-be53-048793bd0e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computer science is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. It encompasses a wide range of topics, from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software. The field includes areas such as theoretical computer science, applied computer science, software engineering, and the study of fundamental computer algorithms.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('What is computer science?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d233dbd-b847-4838-a255-80cbbb775994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Studying computer science is important for several reasons. Firstly, it prepares students for careers in the technology industry, which is experiencing rapid growth and high demand for skilled professionals. Additionally, computer science education promotes computational thinking skills, which are valuable in various fields such as business, healthcare, and education. Furthermore, computer science education fosters problem-solving abilities and critical thinking, making students more effective in addressing real-world challenges. Lastly, the study of computer science is essential in understanding the nature of computation and automation, which is increasingly integrated into all aspects of society.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('Why is it important to study computer science?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6dd3dea-411e-4482-9091-f1f9b5fc7862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, students should still learn about computer science even if machines abstract away coding. Computer science education encompasses a wide range of topics, from basic programming skills to advanced algorithm design and data analysis. It is essential for preparing students for careers in the technology industry and other fields that require computational skills. Additionally, computer science education promotes computational thinking skills, which are valuable in many fields, including business, healthcare, and education. As technology becomes increasingly integrated into all aspects of society, the demand for skilled computer scientists is growing, making computer science education crucial for students in the 21st century workforce.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('Should students still learn about computer science if machines abstract away coding?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44c82507-b545-4bd1-801e-5df652ecc4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the below articles on Computer Science to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Information and computer science\n",
      "\n",
      "==Areas of information and computer science==\n",
      "\n",
      "===Programming theory===\n",
      "\n",
      "The process of taking a given algorithm and encoding it into a language that can be understood and executed by a computer. There are many different types of programming languages and various different types of computers, however, they all have the same goal: to turn algorithms into machine code.\n",
      "\n",
      "Popular programming languages used within the academic study of CIS include, but are not limited to: Java, Python, C#, C++, Perl, Ruby, Pascal, Swift, Visual Basic.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Computer science\n",
      "\n",
      "==Programming paradigms==\n",
      "\n",
      "{{main|Programming paradigm}}\n",
      "\n",
      "Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n",
      "* [[Functional programming]], a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.\n",
      "* [[Imperative programming]], a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\n",
      "* [[Object-oriented programming]], a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.\n",
      "* [[Service-oriented programming]], a programming paradigm that uses \"services\" as the unit of computer work, to design and implement integrated business applications and [[mission critical]] software programs\n",
      "\n",
      "Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "CS50\n",
      "\n",
      "==Follow-up courses==\n",
      "\n",
      "CS50 offers several follow-up courses, including:\n",
      "\n",
      "* CS50 Computer Science for Web Programming - a more in-depth look at [[HTML]], [[CSS]], and [[JavaScript]], as well as frameworks including [[Django (web framework)|Django]] and [[React (software)|React]].\n",
      "* CS50's Introduction to Artificial Intelligence with [[Python (programming language)|Python]] - covers [[search algorithm]]s, [[machine learning]], and [[artificial intelligence]].\n",
      "* CS50's Introduction to Game Development - teaches the [[Unity (game engine)|Unity]] and [[Löve (game engine)|Löve 2D]] [[game engine]]s, as well as [[2-dimensions|2D]] and [[3D computer graphics|3D]] game principles.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Lua (programming language)\n",
      "\n",
      "== Applications ==\n",
      "\n",
      "{{main|List of applications using Lua}}\n",
      "\n",
      "In [[video game development]], Lua is widely used as a [[scripting language]] by [[Video game programmer#Scripter|programmers]], mainly due to its perceived easiness to embed, fast execution, and short [[learning curve]]. Notable games which use Lua include ''[[Roblox]]'', ''[[Garry's Mod]]'', '' [[World of Warcraft]]'', ''[[Payday 2]]'',  ''[[Phantasy Star Online 2]]'', ''[[Dota 2]]'', ''[[Crysis (video game)|Crysis]]'', and many others. Some games that do not natively support Lua programming or scripting, have this functionality added by mods, such as ComputerCraft does for ''[[Minecraft]]''. In addition, Lua is also used in non-video game software, such as [[Adobe Lightroom]], [[Moho (software)|Moho]], [[iClone]], [[Aerospike (database)|Aerospike]] and certain system software in [[FreeBSD]] and [[NetBSD]], and is used as a template scripting language on [[MediaWiki]] using the Scribunto extension.\n",
      "\n",
      "In 2003, a poll conducted by GameDev.net showed Lua was the most popular scripting language for game programming. On 12 January 2012, Lua was announced as a winner of the Front Line Award 2011 from the magazine ''[[Game Developer (magazine)|Game Developer]]'' in the category Programming Tools.\n",
      "\n",
      "A large number of non-game applications also use Lua for extensibility, such as [[LuaTeX]], an implementation of the [[TeX]] type-setting language, [[Redis]], a [[key-value database]], [[Neovim]], a text editor, [[Nginx]], a [[web server]], and [[Wireshark]], a network packet analyzer.\n",
      "\n",
      "Through the Scribunto extension, Lua is available as a server-side scripting language in the [[MediaWiki]] software that powers [[Wikipedia]] and other wikis. Among its uses are allowing the integration of data from [[Wikidata]] into articles, and powering the {{srlink|Wikipedia:Automated taxobox system|automated taxobox system}}.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Lua (programming language)\n",
      "\n",
      "== Features ==\n",
      "\n",
      "{{Wikibooks|Lua Programming}}\n",
      "\n",
      "Lua is commonly described as a \"[[multi-paradigm programming language|multi-paradigm]]\" language, providing a small set of general features that can be extended to fit different problem types. Lua does not contain explicit support for [[Inheritance (object-oriented programming)|inheritance]], but allows it to be implemented with [[metatable]]s. Similarly, Lua allows programmers to implement [[namespace]]s, [[Class (computer programming)|classes]], and other related features using its single table implementation; [[first-class function]]s allow the employment of many techniques from [[functional programming]]; and full [[lexical scoping]] allows fine-grained [[information hiding]] to enforce the [[principle of least privilege]].\n",
      "\n",
      "In general, Lua strives to provide simple, flexible [[metaprogramming|meta-features]] that can be extended as needed, rather than supply a feature-set specific to one programming paradigm. As a result, the base language is [[Lightweight programming language|light]]—the full reference [[interpreter (computing)|interpreter]] is only about 247&nbsp;[[Kilobyte|kB]] compiled<ref name=luaabout />—and easily adaptable to a broad range of applications.\n",
      "\n",
      "A [[dynamically typed]] language intended for use as an extension language or [[scripting language]], Lua is compact enough to fit on a variety of host platforms. It supports only a small number of atomic data structures such as [[Boolean data type|Boolean]] values, numbers (double-precision [[floating point]] and 64-bit [[integer]]s by default), and [[string (computer science)|strings]]. Typical data structures such as [[Array data structure|arrays]], [[set (computer science)|sets]], [[List (computing)|lists]], and [[record (computer science)|records]] can be represented using Lua's single native data structure, the table, which is essentially a heterogeneous [[associative array]].\n",
      "\n",
      "Lua implements a small set of advanced features such as [[first-class function]]s, [[garbage collection (computer science)|garbage collection]], [[Closure (computer science)|closures]], proper [[Tail recursion|tail calls]], [[Type conversion|coercion]] (automatic conversion between string and number values at run time), [[coroutine]]s (cooperative multitasking) and [[Dynamic loading|dynamic module loading]].\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Lego Mindstorms\n",
      "\n",
      "==Programming languages==\n",
      "\n",
      "| Based on NXT_Python, includes additional advanced features, support for around 30 sensors, and multiple brick connection backends. Works on Windows, Linux, Mac.\n",
      "|\n",
      "|-\n",
      "| NXTGCC\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| Assembly, [[C (programming language)|C]], makefiles, [[Eclipse (software)|Eclipse]], etc.\n",
      "| The first GCC toolchain for programming the Lego Mindstorms NXT firmware.\n",
      "|\n",
      "|-\n",
      "| [[nxtOSEK]]\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| [[C (programming language)|C]]/[[C++]]\n",
      "|\n",
      "|\n",
      "|-\n",
      "| OCaml-mindstorm\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| [[OCaml]]\n",
      "| Module to control Lego NXT robots using OCaml through the Bluetooth and USB interfaces.\n",
      "|\n",
      "|-\n",
      "| OnScreen\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| A custom language which can be programmed directly on the RCX\n",
      "|\n",
      "|\n",
      "|-\n",
      "| pbForth\n",
      "|{{Yes}}\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| [[Forth (programming language)|Forth]]\n",
      "|No longer developed.\n",
      "|\n",
      "|-\n",
      "|pbLua\n",
      "|\n",
      "|{{Yes}}\n",
      "|\n",
      "|\n",
      "|\n",
      "|API for the [[Lua (programming language)|Lua]] programming language for the Mindstorms NXT, text-based\n",
      "|pBLua: ... is written in portable C, with minimal runtime requirements; can be compiled on the fly on NXT; is a small, easy to read, and easy to write language; has extensive documentation available online and in dead-tree format, and a very friendly newsgroup\n",
      "|[https://web.archive.org/web/20160129011601/http://hempeldesigngroup.com/lego/pbLua/ website]\n",
      "|-\n",
      "| PBrickDev\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| PBrickDev, a flowchart based language.\n",
      "| Has more function than the RIS language, such as datalogs and subroutines, multithreading\n",
      "|\n",
      "|-\n",
      "| PRO-BOT\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| A kind of Visual Basic/spirit.ocx-based language\n",
      "| Designed for robots which are in contact with the workstation at all times\n",
      "|\n",
      "|-\n",
      "| Processing\n",
      "|\n",
      "|{{Yes}}\n",
      "|\n",
      "|\n",
      "|\n",
      "| Java (simplified, programmed C-style)\n",
      "| [[Processing (programming language)]] is an open source programming language and environment to program images, animation, and interactions. It is used by students, artists, designers, researchers, and hobbyists for learning, prototyping, and production; the NXTComm Processing library developed by Jorge Cardoso can control the NXT with Processing\n",
      "|\n",
      "|-\n",
      "| QuiteC\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| C\n",
      "| A library for use with GCC and comes with GCC for Windows.\n",
      "|\n",
      "|-\n",
      "| RCX Code\n",
      "|{{Yes}}\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| RCX Code, a custom flowchart-based language\n",
      "| Included in the Mindstorms consumer version sold at toystore\n",
      "|\n",
      "|-\n",
      "| ROBOLAB\n",
      "|{{Yes}}\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| A flowchart language based on LabVIEW\n",
      "| This is the programming environment offered to schools who use MindStorms, supports the Lego Cam. The programming structure simulates a flowchart design structure almost icon by icon. Therefore, it helps users a great deal in terms of translating from a flowchart design to Robolab icons. \n",
      "|\n",
      "|-\n",
      "| [[RoboMind]]\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| Simple educational scripting language available from Arabic to Ukrainian.\n",
      "| The RoboMind learning environment allows to quickly develop and test scripts for a virtual robot environment. The scripts can then directly be transferred to a Lego Mindstorms NXT robot. It works on the standard firmware.\n",
      "|\n",
      "|-\n",
      "| RoboRealm\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "| A multi-platform language that works with IRobot Roomba, NXT, RCX, VEX, and many other popular robotic sets; can also process video using a webcam, this gives a robot excellent vision since it can filter out certain colors, lock-on to a certain area of color, display variables from the robot or computer, and much more; software works with keyboard, joystick, mouse\n",
      "|\n",
      "|\n",
      "|-\n",
      "|Robot JavaScript\n",
      "|\n",
      "|{{No}}\n",
      "|{{Yes}}\n",
      "|{{Yes}}\n",
      "|{{Yes}}\n",
      "|JavaScript\n",
      "|A compiler that compiles JavaScript code for EV3 robots. Includes syntax highlighting, code sharing, over 100 example programs, and verbose compiler messages. Object oriented language. Free.\n",
      "|[https://RobotJavaScript.com website]\n",
      "|-\n",
      "| [[ROBOTC]]\n",
      "|{{Yes}}\n",
      "|{{Yes}}\n",
      "|{{Yes}}\n",
      "|{{Yes}}\n",
      "|\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "P4 (programming language)\n",
      "\n",
      "==Design==\n",
      "\n",
      "===Reconfigurability===\n",
      "\n",
      "Protocol independence and the abstract language model allow for reconfigurability–P4 targets should be able to change the way they process packets (perhaps multiple times) after they are deployed.  This capability is traditionally associated with forwarding planes built on [[History of general-purpose CPUs|general-purpose CPUs]] or [[network processor]]s, rather than the fixed function [[Application-specific integrated circuit|ASICs]]. Although within the language there is nothing to prevent a given target from optimizing around a certain set of protocols, these optimizations are invisible to the language author and may ultimately reduce the system's flexibility and reconfigurability goals.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Lua (programming language)\n",
      "\n",
      "== Derived languages ==\n",
      "\n",
      "===Languages that compile to Lua===\n",
      "\n",
      "* MoonScript is a [[Dynamic programming language|dynamic]], [[Whitespace character|whitespace]]-sensitive [[scripting language]] inspired by [[CoffeeScript]], which is compiled into Lua. This means that instead of using <code>do</code> and <code>end</code> (or <code>{</code> and <code>}</code>) to delimit sections of code it uses [[Line break (computing)|line breaks]] and [[indentation style]]. A notable usage of MoonScript is the video game distribution website [[Itch.io]].\n",
      "* [[Haxe]] supports compilation to a Lua target, supporting Lua 5.1-5.3 as well as [[LuaJIT]] 2.0 and 2.1.\n",
      "* Fennel, a Lisp dialect that targets Lua.<ref name=\":1\" />\n",
      "* Urn, a [[Lisp (programming language)|Lisp]] dialect that is built on Lua.\n",
      "* Amulet, an [[ML (programming language)|ML]]-like [[Functional programming|functional language]], whose compiler outputs Lua files.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Generation gap (pattern)\n",
      "\n",
      "== Modern languages ==\n",
      "\n",
      "Modern byte-code language like Java were in their early stages when Vlissides developed his ideas. In a language like Java or C#, this pattern may be followed by generating an interface, which is a completely abstract class. The developer would then hand-modify a concrete implementation of the generated interface.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "Skeleton (computer programming)\n",
      "\n",
      "== Implementation ==\n",
      "\n",
      "Skeleton programming can be implemented in a range of different programming applications.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "String literal\n",
      "\n",
      "== Embedding source code in string literals ==\n",
      "\n",
      "Languages that lack flexibility in specifying string literals make it particularly cumbersome to write programming code that generates other programming code. This is particularly true when the generation language is the same or similar to the output language.\n",
      "\n",
      "For example:\n",
      "* writing code to produce [[Quine (computing)|quine]]s\n",
      "* generating an output language from within a [[web template]];\n",
      "* using [[XSLT]] to generate XSLT, or [[SQL]] to generate more SQL\n",
      "* generating a [[PostScript]] representation of a document for printing purposes, from within a document-processing application written in [[C (programming language)|C]] or some other language.\n",
      "* writing [[shader]]s\n",
      "\n",
      "Nevertheless, some languages are particularly well-adapted to produce this sort of self-similar output, especially those that support multiple options for avoiding delimiter collision.\n",
      "\n",
      "Using string literals as code that generates other code may have adverse security implications, especially if the output is based at least partially on untrusted user input. This is particularly acute in the case of Web-based applications, where malicious users can take advantage of such weaknesses to subvert the operation of the application, for example by mounting an [[SQL injection]] attack.\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "P4 (programming language)\n",
      "\n",
      "==Design==\n",
      "\n",
      "As the language is specifically targeted at packet forwarding applications, the list of requirements or design choices is somewhat specific to those use cases. The language is designed to meet several goals:\n",
      "\"\"\"\n",
      "\n",
      "Wikipedia article section:\n",
      "\"\"\"\n",
      "P4 (programming language)\n",
      "\n",
      "==Design==\n",
      "\n",
      "===Protocol independence===\n",
      "\n",
      "P4 is designed to be protocol-independent: the language has no native support for even common protocols such as IP, Ethernet, TCP, [[Virtual Extensible LAN|VxLAN]], or [[Multiprotocol Label Switching|MPLS]].  Instead, the P4 programmer describes the header formats and field names of the required protocols in the program, which are in turn interpreted and processed by the compiled program and target device.\n",
      "\"\"\"\n",
      "\n",
      "Question: What is the most versatile coding language?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most versatile coding language would be Lua. Lua is commonly described as a \"multi-paradigm\" language, providing a small set of general features that can be extended to fit different problem types. It supports a range of programming paradigms and features, such as first-class functions, metaprogramming, dynamic typing, and a small number of atomic data structures. Additionally, Lua can be used as a scripting language, embedded in other applications, and compiled into other languages. Therefore, Lua\\'s versatility makes it well-suited for various types of programming tasks and applications.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('What is the most versatile coding language?', print_message =True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
